# ç¯å¢ƒé…ç½®

- [win10/11ä¸‹wsl2å®‰è£…gpuç‰ˆçš„pytorchï¼ˆé¿å‘æŒ‡å—ï¼‰ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/488731878)
- [WSL2 å®‰è£… CUDA + PyTorch - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/506477744)

## -1 Nvidiaé©±åŠ¨

[Official Drivers | NVIDIA](https://www.nvidia.com/Download/index.aspx?lang=en-us)

å‹å·è¦å¯¹åº”ä¸Šï¼Œä¸‹è½½é»˜è®¤ Game Ready Driver é©±åŠ¨å³å¯
## 0 WSL

```shell
wsl --install Ubuntu-20.04
sudo passwd
su
/usr/lib/wsl/lib/nvidia-smi # æŸ¥çœ‹æ˜¾å¡æƒ…å†µ
```

ç¼–è¾‘ `~/.bashrc`ï¼Œæ·»åŠ  

```
export PATH=/usr/lib/wsl/lib:$PATH
```
## 1 (å¯é€‰)Docker

```shell
docker pull pytorch/pytorch
docker run -it -v E:\Docker\Chatglm:/chatglm pytorch/pytorch
apt update
apt install wget vim -y
```

dockeré…ç½®æ–‡ä»¶å­˜åœ¨ `wsl$\\docker-desktop-data\data\docker\containers`

## 2 CUDA

```
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
apt update
apt install cuda-toolkit-12-4
```

ç¼–è¾‘ `~/.bashrc`ï¼Œæ·»åŠ 

```text
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PAT
```

è¿è¡Œ `nvcc -V` ä¼šæœ‰CUDAç‰ˆæœ¬çš„è¾“å‡º

## 3 PyTorch

```shell
pip install python3-pip
pip install torch torchtext torchvision sentencepiece psutil future torchserve torch-model-archiver
```

## 4 æ¨¡å‹

- [THUDM/chatglm3-6b-128k Â· Hugging Face](https://huggingface.co/THUDM/chatglm3-6b-128k)
- [å¦‚ä½•ä½¿ç”¨ğŸ¤—hugging faceçš„æ¨¡å‹åº“? - æ˜é‡‘ (juejin.cn)](https://juejin.cn/post/7225425984311820348)

```shell
pip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

> æ³¨ï¼šå¯èƒ½å‘½ä»¤å¤ªé•¿äº†ï¼Œç›´æ¥æ‰”è¿›å»ä¼šå¡ä½ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ†ä¸€ä¸‹

è¾“å…¥ `python3` è¿›å…¥äº¤äº’æ¨¡å¼
```ipython
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b-128k", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm3-6b-128k", trust_remote_code=True) 
model.save_pretrained('./chatglm3') # å­˜æ”¾
model = model.half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
```

åŠ è½½æ¨¡å‹

```ipython
model = AutoModel.from_pretrained('./chatglm3')
```