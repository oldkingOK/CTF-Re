# 环境配置

- [win10/11下wsl2安装gpu版的pytorch（避坑指南） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/488731878)
- [WSL2 安装 CUDA + PyTorch - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/506477744)

## -1 Nvidia驱动

[Official Drivers | NVIDIA](https://www.nvidia.com/Download/index.aspx?lang=en-us)

型号要对应上，下载默认 Game Ready Driver 驱动即可
## 0 WSL

```shell
wsl --install Ubuntu-20.04
sudo passwd
su
/usr/lib/wsl/lib/nvidia-smi # 查看显卡情况
```

编辑 `~/.bashrc`，添加 

```
export PATH=/usr/lib/wsl/lib:$PATH
```
## 1 (可选)Docker

```shell
docker pull pytorch/pytorch
docker run -it -v E:\Docker\Chatglm:/chatglm pytorch/pytorch
apt update
apt install wget vim -y
```

docker配置文件存在 `wsl$\\docker-desktop-data\data\docker\containers`

## 2 CUDA

```
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
apt update
apt install cuda-toolkit-12-4
```

编辑 `~/.bashrc`，添加

```text
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PAT
```

运行 `nvcc -V` 会有CUDA版本的输出

## 3 PyTorch

```shell
pip install python3-pip
pip install torch torchtext torchvision sentencepiece psutil future torchserve torch-model-archiver
```

## 4 模型

- [THUDM/chatglm3-6b-128k · Hugging Face](https://huggingface.co/THUDM/chatglm3-6b-128k)
- [如何使用🤗hugging face的模型库? - 掘金 (juejin.cn)](https://juejin.cn/post/7225425984311820348)

```shell
pip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate
```

> 注：可能命令太长了，直接扔进去会卡住，可以手动分一下

输入 `python3` 进入交互模式
```ipython
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b-128k", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm3-6b-128k", trust_remote_code=True) 
model.save_pretrained('./chatglm3') # 存放
model = model.half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。
response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
print(response)
```

加载模型

```ipython
model = AutoModel.from_pretrained('./chatglm3')
```